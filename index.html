9
<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description" content="NExT-GPT: Any-to-Any Multimodal Large Language Model">
    <meta name="keywords" content="text-to-image generation, Large Language Models, scene synthesis">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>NExT-GPT</title>

    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }

        gtag('js', new Date());

        gtag('config', 'G-PYVRSFMDRL');
    </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <link rel="icon" href="./static/images/logo.png">
    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <link rel="stylesheet" href="./static/css/index-gradio.css">
    <link rel="stylesheet" href="./static/css/live_theme.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>

<body>


    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title"
                            style="display: flex;flex-direction: row;align-items: center;justify-content: center;margin-bottom: 5px;">
                            <img src="./static/images/logo.png" width="60" height="60"
                                style="margin-right: 10px;">NExT-GPT:</h1>
                        <h1 class="title is-2 publication-title">Any-to-Any Multimodal Large Language Model</h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a href="https://chocowu.github.io/">Shengqiong Wu</a>,</span>
                            <span class="author-block">
                                <a href="https://haofei.vip/">Hao Fei</a><sup>*</sup>,
                            </span>
                            <span class="author-block">
                                <a href="#">Leigang Qu</a>,</span>
                            <span class="author-block">
                                <a href="https://jiwei0523.github.io/">Wei Ji</a>,</span>
                            <span class="author-block">
                                <a href="https://www.chuatatseng.com/">Tat-Seng Chua</a>
                            </span>
                        </div>

                        <div class="is-size-5 publication-authors" style="margin-top: 10px;">
                            <span class="author-block"><a href="https://www.nextcenter.org/">NExT++ Lab</a>, National
                                University of Singapore</span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block" style="font-size: 15px;">(<sup>*</sup>Correspondence)</span>
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- PDF Link. -->
                                <span class="link-block">
                                    <a href="https://arxiv.org/pdf/2309.05519.pdf"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-file-pdf"></i>
                                        </span>
                                        <span>Paper</span>
                                    </a>
                                </span>

                                <span class="link-block">
                                    <a href="https://acc414b22d6839d28f.gradio.live" target="_blank"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fa fa-laugh"></i>
                                        </span>
                                        <span>Demo</span>
                                    </a>
                                </span>

                                <!-- Code Link. -->
                                <span class="link-block">
                                    <a href="https://github.com/NExT-GPT/NExT-GPT"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github"></i>
                                        </span>
                                        <span>Code</span>
                                    </a>
                                </span>


                                <span class="link-block">
                                    <a href="https://github.com/NExT-GPT/NExT-GPT" target="_blank"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fa fa-database"></i>
                                        </span>
                                        <span>Dataset</span>
                                    </a>
                                </span>

                                <!-- Video Link. -->
                                <span class="link-block">
                                    <a href="https://www.youtube.com/watch?v=aqw2SCWeWD0"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-youtube"></i>
                                        </span>
                                        <span>Video</span>
                                    </a>
                                </span>


                            </div>

                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>


    <section class="section">
        <div class="container is-max-desktop">
            <!-- Paper video. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-full-width">
                    <h2 class="title is-2">Video Presentation</h2>
                    <div class="publication-video">
                        <iframe src="https://www.youtube.com/embed/aqw2SCWeWD0?si=jIKgOkew2rmRROUy" frameborder="0"
                            allow="autoplay; encrypted-media" allowfullscreen></iframe>
                    </div>
                </div>
            </div>
            <!--/ Paper video. -->
        </div>
    </section>


    <section class="section">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-2">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>
                            While recently Multimodal Large Language Models (MM-LLMs) have made exciting strides, they
                            mostly fall prey to the limitation
                            of only input-side multimodal understanding, without the ability to produce content in
                            multiple modalities.
                            As we humans always perceive the world and communicate with people through various
                            modalities, developing any-to-any MM-LLMs
                            capable of accepting and delivering content in any modality becomes essential to human-level
                            AI.
                            To fill the gap, we present an end-to-end general-purpose any-to-any MM-LLM system,
                            <b>NExT-GPT</b>.
                            We connect an LLM with multimodal adaptors and different diffusion decoders, enabling
                            NExT-GPT to perceive inputs and generate
                            outputs in arbitrary combinations of text, images, videos, and audio.
                            By leveraging the existing well-trained highly-performing encoders and decoders, NExT-GPT is
                            tuned with only a small amount of
                            parameter (1%) of certain projection layers, which not only benefits low-cost training and
                            also facilitates convenient
                            expansion to more potential modalities.
                            Moreover, we introduce a modality-switching instruction tuning (MosIT) and manually curate a
                            high-quality dataset for MosIT,
                            based on which NExT-GPT is empowered with complex cross-modal semantic understanding and
                            content generation.
                            Overall, our research showcases the promising possibility of building an AI agent capable of
                            modeling universal modalities,
                            paving the way for more human-like AI research in the community.
                        </p>
                    </div>
                </div>
            </div>
            <!--/ Abstract. -->

            <br>
        </div>
    </section>


    <section class="section">
        <div class="container is-max-desktop">

            <div class="columns is-centered has-text-centered">
                <h2 class="title is-2">Technical Description</h2>
                <br>
            </div>

            <!-- Architecture -->
            <div class="columns is-centered">
                <div class="column is-full-width">
                    <h4 class="title is-3">• Architecture</h4>

                    <div class="content has-text-justified">
                        <img class="columns is-centered has-text-centered" src="./static/images/framework.png"
                            alt="Teaser" width="95%" style="margin:0 auto">
                        <br>
                        <figcaption>
                            <p style="text-align: center;">
                                <font color="061E61">
                                    <b>Figure 1:</b> By connecting LLM with multimodal adaptors and diffusion decoders,
                                    NExT-GPT achieves universal
                                    multimodal understanding and any-to-any modality input and output.
                                </font>
                            </p>
                        </figcaption>
                        <br>
                        <p>
                        <ul>
                            <li>
                                <b>Multimodal Encoding Stage.</b> Leveraging existing well-established models to encode
                                inputs of various modalities.
                                Here we take advantage of the ImageBind, which is a unified high-performance encoder
                                across six modalities.
                                Then, via the linear projection layer, different input representations are mapped into
                                language-like representations that
                                are comprehensible to the LLM.
                            </li>
                            <li>
                                <b>LLM Understanding and Reasoning Stage.</b>
                                An LLM is used as the core agent of NExT-GPT.
                                Technically, we employ the Vicuna.
                                LLM takes as input the representations from different modalities and carries out
                                semantic understanding and reasoning over
                                the inputs.
                                It outputs 1) the textual responses directly, and 2) signal tokens of each modality that
                                serve as instructions to dictate
                                the decoding layers whether to generate multimodal contents, and what content to produce
                                if yes.
                            </li>
                            <li>
                                <b>Multimodal Generation Stage.</b>
                                Receiving the multimodal signals with specific instructions from LLM (if any), the
                                Transformer-based output projection
                                layers map the signal token representations into the ones that are understandable to
                                following multimodal decoders.
                                Technically, we employ the current off-the-shelf latent conditioned diffusion models of
                                different modal generations, i.e.,
                                Stable Diffusion (SD) for image synthesis, Zeroscope for video synthesis, and AudioLDM
                                for audio synthesis.
                            </li>
                        </ul>
                        </p>
                        <br>

                        <img class="columns is-centered has-text-centered" src="./static/images/config.png" alt="Teaser"
                            width="85%" style="margin:0 auto">
                    </div>
                    <br />

                </div>
            </div>

            <!-- Inference -->
            <div class="columns is-centered">
                <div class="column is-full-width">
                    <h2 class="title is-3">• System Inference</h2>

                    <div class="content has-text-justified">
                        <p>
                            In Figure 2 we further illustrate the inference procedure of NExT-GPT.
                            Given certain user inputs of any combination of modalities, the corresponding modal encoders
                            and projectors transform them
                            into feature representations and passed to LLM (except the text inputs, which will be
                            directly fed into LLM).
                            Then, LLM decides what content to generate, i.e., textual tokens, and modality signal
                            tokens.
                            If LLM identifies a certain modality content (except language) to be produced, a special
                            type of token will be output
                            indicating the activation of that modality; otherwise, no special token output means
                            deactivation of that modality.
                            Technically, we design the '&lt;IMG<sub>i</sub>&gt;' (i=0,...,4) as image signal tokens;
                            '&lt;AUD<sub>i</sub>&gt;' (i=0,...,8) as audio signal tokens; and
                            '&lt;VID<sub>i</sub>&gt;' (i=0,...,24) as video signal tokens.
                            After LLM, the text responses are output to the user; while the representations of the
                            signal tokens of certain activated
                            modalities are passed to the corresponding diffusion decoders for content generation.
                        </p>
                        <br>
                        <img class="columns is-centered has-text-centered" src="./static/images/inference.png"
                            alt="Teaser" width="100%" style="margin:0 auto">
                        <br>
                        <figcaption>
                            <p style="text-align: center;">
                                <font color="061E61">
                                    <b>Figure 2:</b> NExT-GPT inference process. Grey colors denote the deactivation of
                                    the modules.
                                </font>
                            </p>
                        </figcaption>
                    </div>
                    <br />

                </div>
            </div>

            <!-- Alignment -->
            <div class="columns is-centered">
                <div class="column is-full-width">
                    <h2 class="title is-3">• Lightweight Multimodal Alignment Learning</h2>

                    <div class="content has-text-justified">
                        <p>
                            We design the system with mainly three tiers in loose coupling, and we only need to update
                            the two projection layers at
                            encoding side and decoding side.
                        <ul>
                            <li>
                                <b>Encoding-side LLM-centric Multimodal Alignment.</b>
                                We align different inputting multimodal features with the text feature space, the
                                representations that are understandable
                                to the core LLM.
                            </li>
                            <li>
                                <b>Decoding-side Instruction-following Alignment.</b>
                                We minimize the distance between the LLM's modal signal token representations (after
                                each Transformer-based project layer)
                                and the conditional text representations of the diffusion models.
                                Since only the textual condition encoders are used (with the diffusion backbone frozen),
                                the learning is merely based on
                                the purely captioning texts, i.e., without any visual or audio inputs.
                            </li>
                        </ul>
                        </p>
                        <img class="columns is-centered has-text-centered" src="./static/images/alignment.png"
                            alt="Teaser" width="100%" style="margin:0 auto">
                        <br>
                        <figcaption>
                            <p style="text-align: center;">
                                <font color="061E61">
                                    <b>Figure 3:</b> Illustration of the lightweight multimodal alignment learning of
                                    encoding and decoding.
                                </font>
                            </p>
                        </figcaption>
                    </div>
                    <br />

                </div>
            </div>


            <!-- Instruction -->
            <div class="columns is-centered">
                <div class="column is-full-width">
                    <h2 class="title is-3">• Modality-switching Instruction Tuning (MosIT)</h2>

                    <div class="content has-text-justified">
                        <p>
                            Further instruction tuning (IT) is necessary to enhance the capabilities and controllability
                            of LLM.
                            To facilitate the development of any-to-any MM-LLM, we propose a novel Modality-switching
                            Instruction Tuning (<b>MosIT</b>).
                            As illustrated in Figure 4, when an IT dialogue sample is fed into the system, the LLM
                            reconstructs and generates the textual
                            content of input (and represents the multimodal content with the multimodal signal tokens).
                            The optimization is imposed based on gold annotations and LLM's outputs.
                            In addition to the LLM tuning, we also fine-tune the decoding end of NExT-GPT.
                            We align the modal signal token representation encoded by the output projection with the
                            gold multimodal caption
                            representation encoded by the diffusion condition encoder.
                            Thereby, the comprehensive tuning process brings closer to the goal of faithful and
                            effective interaction with users.
                        </p>
                        <br>
                        <img class="columns is-centered has-text-centered" src="./static/images/instruction_tuning.png"
                            alt="Teaser" width="100%" style="margin:0 auto">
                        <br>
                        <figcaption>
                            <p style="text-align: center;">
                                <font color="061E61">
                                    <b>Figure 4:</b> Illustration of modality-switching instruction tuning.

                                </font>
                            </p>
                        </figcaption>
                    </div>
                    <br />

                    <h4 class="title is-4">MosIT Data</h4>
                    <p>
                        All the existing IT datasets fail to meet the requirements for our any-to-any MM-LLM scenario.
                        We thus construct the <b>MosIT</b> dataset of high quality.
                        The data encompasses a wide range of multimodal inputs and outputs, offering the necessary
                        complexity and variability to
                        facilitate the training of MM-LLMs that can handle diverse user interactions and deliver desired
                        responses accurately.
                    </p>
                    <img class="columns is-centered has-text-centered" src="./static/images/MosIT.png" alt="Teaser"
                        width="100%" style="margin:0 auto">

                </div>
            </div>

        </div>
    </section>


    <section class="section">
        <div class="container is-max-desktop">

            <!--    <div class="columns is-centered has-text-centered">-->
            <!--        <h2 class="title is-2">Demonstrations</h2>-->
            <!--      <br>-->
            <!--    </div>-->

            <div class="columns is-centered">
                <div class="column is-full-width">
                    <h2 class="title is-3">Related Links</h2>

                    <div class="content has-text-justified">
                        <p>
                            You may refer to related work that serves as foundations for our framework and code
                            repository,
                            such as <a href="https://github.com/lm-sys/FastChat">Vicuna</a>,
                            <a href="https://github.com/facebookresearch/ImageBind">ImageBind</a>,
                            <a href="https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/text2img">Stable
                                Diffusion</a>,
                            <a href="https://github.com/haoheliu/AudioLDM">AudioLDM</a>,
                            and <a href="https://huggingface.co/cerspense/zeroscope_v2_576w">Zeroscope</a>.
                            We also partially draw inspirations from <a href="https://codi-gen.github.io/">CoDi</a>,
                            <a href="https://vpgtrans.github.io/">VPGTrans</a>,
                            <a href="https://github.com/DAMO-NLP-SG/Video-LLaMA">Video-LLaMA</a>,
                            <a href="https://github.com/yxuansu/PandaGPT">PandaGPT</a>,
                            <a href="https://github.com/kohjingyu/gill">GILL</a>,
                            and <a href="https://github.com/Vision-CAIR/MiniGPT-4">MiniGPT-4</a>.

                        </p>
                    </div>
                </div>
            </div>

        </div>
    </section>


    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">BibTeX</h2>
            <pre><code>@articles{wu2023nextgpt,
  title={NExT-GPT: Any-to-Any Multimodal LLM},
  author={Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, Tat-Seng Chua},
  journal = {CoRR},
  volume = {abs/2309.05519},
  year={2023}
}
</code></pre>
        </div>
    </section>


    <footer class="footer">
        <div class="container">
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content">
                        <p style="text-align: center;">
                            The webpage is built based on <a
                                href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </footer>

</body>

</html>